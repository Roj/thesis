{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('../pyprot/')\n",
    "import pyprot.graph_models as graph_models\n",
    "from pyprot.downloader import PdbDownloader, ConsurfDBDownloader\n",
    "from pyprot.protein import Protein\n",
    "from pyprot.structure import Perseus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph verification and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import pyprot.constants\n",
    "def load_graph(fn):\n",
    "    with open(fn, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "filenames = [fn for fn in os.listdir(\"graphs/\") if \".pkl\" in fn]\n",
    "graphs = [load_graph(\"graphs/\"+fn) for fn in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_amino = {code3:i for i, code3 in enumerate(pyprot.constants.AMINOACIDS_3)}\n",
    "index_amino[\"UNK\"] = len(index_amino)\n",
    "num_amino = len(index_amino)\n",
    "\n",
    "# Features are aminoacid type, bfactor and x,y,z coord.\n",
    "all_features = []\n",
    "for graph in graphs:\n",
    "    features = np.zeros((graph.number_of_nodes(), num_amino + 4))\n",
    "    for i, node_idx in enumerate(graph.nodes):\n",
    "        node = graph.nodes[node_idx]\n",
    "        features[i, index_amino[node[\"resname\"]]] = 1\n",
    "        features[i, num_amino] = node[\"bfactor\"]\n",
    "        features[i, num_amino+1:num_amino+4] = node[\"coord\"]\n",
    "    all_features.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adj = [nx.adjacency_matrix(graph) for graph in graphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def touches_ligand(x):\n",
    "    return x <= 4 or (x<=6 and np.random.binomial(1, 1-(x-4)/2) == 1)\n",
    "\n",
    "class_balance = []\n",
    "all_targets = []\n",
    "for graph in graphs:\n",
    "    targets = np.zeros((graph.number_of_nodes(), 2))\n",
    "    for i, node_idx in enumerate(graph.nodes):\n",
    "        distance = graph.nodes[node_idx][\"distance\"]\n",
    "        targets[i, 0] = 1 if touches_ligand(distance) else 0\n",
    "        targets[i, 1] = 1 - targets[i, 0]\n",
    "    class_balance.append(targets[:, 0].sum() / targets[:, 1].sum())\n",
    "    all_targets.append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For every non-contact point there are {} contact points\".format(\n",
    "    sum(class_balance)/len(class_balance)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggyG = load_graph(\"graphs/4DX2.pkl\")\n",
    "distances = []\n",
    "for node_idx in buggyG.nodes:\n",
    "    distances.append(buggyG.nodes[node_idx][\"distance\"])\n",
    "sorted(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check errors\n",
    "pdb_error_list = []\n",
    "for i,target in enumerate(all_targets):\n",
    "    if target[:,0].sum() < 1.0:\n",
    "        print(\"Error found in target #{}\".format(i))\n",
    "        pdb_error_list.append((i,filenames[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_error_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import gcn.utils\n",
    "import gcn.models\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splits and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_adj = [gcn.utils.sparse_to_tuple(gcn.utils.normalize_adj(adj)) \n",
    "            for adj in all_adj]\n",
    "all_features = [gcn.utils.preprocess_features(sp.lil_matrix(features))\n",
    "            for features in all_features]\n",
    "\n",
    "nb_nodes = max(map(lambda adj: adj[2][1], all_adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make matrices the same size\n",
    "for i, adj_tuple in enumerate(all_adj):\n",
    "    #adj_tuple[2] is the shape, and we want it to be always the same..\n",
    "    all_adj[i] = (adj_tuple[0], adj_tuple[1], (nb_nodes, nb_nodes))\n",
    "\n",
    "for i, feat_tuple in enumerate(all_features):\n",
    "    #adj_tuple[2] is the shape, and we want it to be always the same..\n",
    "    all_features[i] = (feat_tuple[0], feat_tuple[1], (nb_nodes, feat_tuple[2][1]))\n",
    "\n",
    "    \n",
    "for i, target in enumerate(all_targets):\n",
    "    padded = np.zeros((nb_nodes, 2))\n",
    "    padded[:target.shape[0], 0] = target[:, 0]\n",
    "    padded[:target.shape[0], 1] = 1 - target[:, 0]\n",
    "    all_targets[i] = padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(data, train_perc, val_perc):\n",
    "    num_train = int(len(data) * train_perc)\n",
    "    num_val = int(len(data) * val_perc)\n",
    "    return data[:num_train], data[num_train:num_train+num_val], data[num_train+num_val:]\n",
    "\n",
    "features_train, features_val, features_test = split_list(all_features, 0.70, 0.15)\n",
    "adj_train, adj_val, adj_test = split_list(all_adj, 0.70, 0.15)\n",
    "y_train, y_val, y_test = split_list(all_targets, 0.70, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning Rate\")\n",
    "flags.DEFINE_integer(\"epochs\", 200, \"Epochs\")\n",
    "flags.DEFINE_integer(\"hidden1\", 16, \"Num units in HL1\")\n",
    "flags.DEFINE_float(\"dropout\", 0.5, \"Dropout\")\n",
    "flags.DEFINE_float(\"weight_decay\", 5e-4, \"Weight decay\")\n",
    "flags.DEFINE_integer(\"early_stopping\", 10, \"Tolerance\")\n",
    "flags.DEFINE_integer(\"max_degree\", 3, \"Max chebyshev polynomial degree\")\n",
    "flags.sys.argv = flags.sys.argv[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholders = {\n",
    "    \"support\": [tf.sparse_placeholder(tf.float32, name=\"support\")],\n",
    "    \"features\": tf.sparse_placeholder(tf.float32, \n",
    "        #shape=tf.constant(features_train[0][2], dtype=tf.int64, name=\"feat_shape_const\"),\n",
    "        name=\"features\"),\n",
    "    \"labels\": tf.placeholder(tf.float32, shape=(nb_nodes, 2), name=\"labels\"),\n",
    "    \"labels_mask\": tf.placeholder(tf.int32, name=\"labels_mask\"),\n",
    "    \"dropout\": tf.placeholder_with_default(0., shape=(), name=\"dropout\"),\n",
    "    \"num_features_nonzero\": tf.placeholder(tf.int32, name=\"nfn0\")\n",
    "}\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = gcn.utils.construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedGCN(gcn.models.GCN):\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = tf.metrics.auc(\n",
    "            tf.argmax(self.outputs, 1), \n",
    "            tf.argmax(self.placeholders['labels'], 1),\n",
    "            self.placeholders['labels_mask']\n",
    "        )\n",
    "model = BalancedGCN(placeholders, input_dim=features_train[0][2][1], logging=True)\n",
    "sess = tf.Session()\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "cost_val = []\n",
    "mask = np.ones((nb_nodes))\n",
    "\n",
    "for epoch in range(FLAGS.epochs):\n",
    "    # Train step\n",
    "    train_loss_total = 0\n",
    "    train_acc_total = 0\n",
    "    for i, features in enumerate(features_train):\n",
    "        support = [adj_train[i]]\n",
    "        train_mask = mask\n",
    "        feed_dict = gcn.utils.construct_feed_dict(\n",
    "            features, support, y_train[i], train_mask, placeholders)\n",
    "        feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "        outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "        \n",
    "        train_loss_total += outs[1]\n",
    "        train_acc_total += outs[2][1]\n",
    "    train_loss = train_loss_total / len(features_train)\n",
    "    train_acc = train_acc_total / len(features_train)\n",
    "    \n",
    "    # Validation step\n",
    "    val_loss_total = 0\n",
    "    val_acc_total= 0\n",
    "    for i, features in enumerate(features_val):\n",
    "        support = [adj_val[i]]\n",
    "        val_mask = mask\n",
    "        loss, acc, duration = evaluate(features, support, y_val[i], val_mask, placeholders)\n",
    "        val_loss_total += loss\n",
    "        val_acc_total += acc[1]\n",
    "    val_loss = val_loss_total / len(features_val)\n",
    "    val_acc = val_acc_total / len(features_val)\n",
    "    \n",
    "    cost_val.append(val_loss)\n",
    "    print(\"Epoch: {}, train_loss={:.5f} train_acc={:.5f} \\\n",
    "val_loss={:.5f} val_acc={:.5f}\".format(epoch+1, train_loss, train_acc,\n",
    "                                              val_loss, val_acc))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
